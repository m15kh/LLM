{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "# Load API configuration from a YAML file\n",
    "with open(\"/home/ubuntu/m15kh/own/LLM/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = config.get(\"LANGSMITH_TRACING\", \"false\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = config[\"LANGSMITH_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = config[\"TAVILY_API_KEY\"]\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = config['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889c56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'President of Iran - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/President_of_Iran', 'content': 'The first election was held in 1980 and was won by Abulhassan Banisadr. Masoud Pezeshkian currently serves as the president of Iran, after being elected in the', 'score': 0.8839694}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 1)\n",
    "question = 'who is the president of iran'\n",
    "search_results = search.invoke(question)\n",
    "print(search_results)\n",
    "tools = [search]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b28ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='Qwen/Qwen3-0.6B',\n",
    "    task = 'text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens = 512,\n",
    "        do_sample = False,\n",
    "        repetition_penalty = 1.03\n",
    "    )\n",
    "    \n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7da8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You're a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "hi<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user just said \"hi\". I need to respond appropriately. Since they didn't ask a specific question, I should acknowledge their greeting. Maybe say something like \"Hello! How can I assist you today?\" That's friendly and opens the door for further conversation. Let me check if there's any other thing they might need, but since there's no context, keeping it simple is best.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"hi\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "\n",
    "\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec70e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: <|im_start|>user\n",
      "Hi!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user just said \"Hi!\" so I need to respond appropriately. Let me think about the best way to greet them. Since they're just saying \"Hi,\" a simple and friendly response would be good. Maybe something like \"Hello! How can I assist you today?\" That's polite and opens the door for conversation. I should make sure to keep it concise and positive. No need for any extra information unless they ask something specific. Let me double-check if there's any cultural nuance I should consider, but since it's a general greeting, it's probably fine. Alright, time to send that response.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? ðŸ˜Š\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "source": [
    "model_with_tools = chat_model.bind_tools(tools)\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5a5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: <|im_start|>user\n",
      "who is the president of iran<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Okay, the user is asking who the President of Iran is. First, I need to confirm that the information is accurate. I remember that Iran's current President is Hassan Khatami. However, I should check if there are any recent changes or updates. Since the user might be referring to the most recent information, I should mention that the President is Hassan Khatami. Also, it's important to note that the President is elected by theå…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼š (People's Congress), which is a key institution in Iran's political system. I need to make sure the answer is clear and concise, avoiding any confusion about the election process.\n",
      "</think>\n",
      "\n",
      "The current President of Iran is **Hassan Khatami**. He was elected in 2023 as the 14th President of Iran. The President is elected by the **People's Congress** (å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼š) of Iran.\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=question)])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
